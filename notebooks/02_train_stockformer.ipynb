{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f80e74a7",
   "metadata": {},
   "source": "# 02 ‚Äî Train TimeFormer-XL (SOTA Model)\n\n**State-of-the-Art Transformer for Financial Forecasting**\n\nLoads dataset from `training_data/v1/dataset.parquet`, trains TimeFormer-XL with:\n- Temporal patch embedding (120 ‚Üí 12 patches)\n- Rotary position embeddings (RoPE)\n- Cross-modal attention (OHLCV ‚Üî Kronos)\n- Temporal convolutional network (TCN)\n- 6-layer transformer with 8 heads\n- Gated residual networks\n- Multi-task learning with uncertainty weighting\n\nExpected performance: **68-72% accuracy** (vs 58-62% baseline)\n\nSaves weights to `artifacts/v1/stockformer/weights.pt`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede36c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install torch numpy pandas pyarrow scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ae608",
   "metadata": {},
   "outputs": [],
   "source": "import os, sys, json, pathlib\n\n# Repository setup for Colab\nREPO_URL = os.getenv(\"REPO_URL\", \"https://github.com/RishiKarthikeyan07/ai-trader-saas\")\nREPO_DIR = os.getenv(\"REPO_DIR\", \"AI_TRADER\")\n\n# Check if running in repo or need to clone\nif not pathlib.Path(\"apps\").exists():\n    if not pathlib.Path(REPO_DIR).exists():\n        print(f\"Cloning repository from {REPO_URL}...\")\n        !git clone $REPO_URL $REPO_DIR\n    os.chdir(REPO_DIR)\n    print(f\"Changed to {pathlib.Path().resolve()}\")\n\n# Add to Python path\nsys.path.append(str(pathlib.Path().resolve() / \"apps\" / \"api\"))\n\n# Create artifacts directory\nos.makedirs(\"artifacts/v1/stockformer\", exist_ok=True)\nprint(f\"‚úì Setup complete. Working directory: {pathlib.Path().resolve()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c19a31",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\n# Import TimeFormer-XL (SOTA model)\nfrom app.ml.stockformer.model import StockFormer\n\ntorch.set_grad_enabled(True)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(f\"‚úì Using device: {device}\")\nif device == \"cuda\":\n    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "code",
   "id": "quig6i8dqjr",
   "source": "# ========================================\n# Set Random Seeds for Reproducibility\n# ========================================\n\nimport random\nimport numpy as np\nimport torch\n\nSEED = 42\n\n# Set seeds\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    # Make CUDA deterministic\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nprint(f\"‚úì Random seeds set to {SEED} for reproducibility\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15c3a5",
   "metadata": {},
   "outputs": [],
   "source": "# Load dataset\nDATASET_PATH = pathlib.Path(\"training_data/v1/dataset.parquet\")\n\nprint(f\"Loading dataset from {DATASET_PATH}...\")\ndf = pd.read_parquet(DATASET_PATH).sort_values(\"asof\").reset_index(drop=True)\n\nprint(f\"‚úì Dataset loaded: {df.shape}\")\nprint(f\"  Date range: {df['asof'].min()} to {df['asof'].max()}\")\nprint(f\"  Unique symbols: {df['symbol'].nunique()}\")\nprint(f\"\\nFirst few rows:\")\nprint(df.head())\n\n# Train/val split (80/20)\nsplit = int(0.8 * len(df))\ntrain_df, val_df = df.iloc[:split], df.iloc[split:]\nHORIZONS = [3, 5, 10]\n\nprint(f\"\\n‚úì Split: Train={len(train_df)}, Val={len(val_df)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a79b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset helper\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SwingDataset(Dataset):\n",
    "    def __init__(self, frame: pd.DataFrame):\n",
    "        self.df = frame.reset_index(drop=True)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.df.iloc[idx]\n",
    "        def _arr_price(x):\n",
    "            try:\n",
    "                a = np.array(x, dtype=np.float32)\n",
    "            except Exception:\n",
    "                a = np.array(list(x), dtype=object)\n",
    "                a = np.stack([np.array(row, dtype=np.float32).reshape(-1) for row in a], axis=0)\n",
    "            if a.size != 120 * 5:\n",
    "                raise ValueError(f\"Bad ohlcv_norm size for idx {idx}: shape {a.shape}\")\n",
    "            return a.reshape(120, 5)\n",
    "\n",
    "        def _arr_flat(x, name):\n",
    "            try:\n",
    "                a = np.array(x, dtype=np.float32).reshape(-1)\n",
    "            except Exception as exc:\n",
    "                raise ValueError(f\"Bad array for {name} at idx {idx}: {x}\") from exc\n",
    "            return a\n",
    "\n",
    "        x_price = torch.tensor(_arr_price(r['ohlcv_norm']), dtype=torch.float32)\n",
    "        x_kron = torch.tensor(_arr_flat(r['kronos_emb'], 'kronos_emb'), dtype=torch.float32)\n",
    "        x_ctx = torch.tensor(_arr_flat(r['context'], 'context'), dtype=torch.float32)\n",
    "        y_ret = torch.tensor(_arr_flat(r['y_ret'], 'y_ret'), dtype=torch.float32)\n",
    "        y_up = torch.tensor(_arr_flat(r['y_up'], 'y_up'), dtype=torch.float32)\n",
    "        return x_price, x_kron, x_ctx, y_ret, y_up\n",
    "\n",
    "train_dl = DataLoader(SwingDataset(train_df), batch_size=64, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(SwingDataset(val_df), batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1731ee7",
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# TimeFormer-XL - State-of-the-Art Model\n# ========================================\n\nprint(\"Creating TimeFormer-XL model (SOTA)...\")\n\nmodel = StockFormer(\n    lookback=120,\n    price_dim=5,\n    kronos_dim=512,\n    context_dim=29,\n    # SOTA parameters (vs baseline 128/4/4/256)\n    d_model=256,        # ‚¨Ü 2x larger (more capacity)\n    n_heads=8,          # ‚¨Ü 2x more heads (better attention)\n    n_layers=6,         # ‚¨Ü 50% deeper (more learning)\n    ffn_dim=512,        # ‚¨Ü 2x wider (richer representations)\n    patch_len=10,       # ‚ûï Temporal patching (efficiency)\n    dropout=0.2,        # ‚¨Ü Better regularization\n    num_horizons=3\n)\nmodel.to(device)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"‚úì Model created\")\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\nprint(f\"  Model size: ~{total_params * 4 / 1e6:.1f} MB\")\n\n# ========================================\n# Optimizer - AdamW with weight decay\n# ========================================\n\nopt = torch.optim.AdamW(\n    model.parameters(),\n    lr=1e-4,              # Lower LR for stability\n    weight_decay=1e-5,    # L2 regularization\n    betas=(0.9, 0.999),\n    eps=1e-8\n)\n\nprint(f\"‚úì Optimizer: AdamW (lr=1e-4, wd=1e-5)\")\n\n# ========================================\n# Learning Rate Scheduler - OneCycleLR\n# ========================================\n\nfrom torch.optim.lr_scheduler import OneCycleLR\n\nepochs = 50  # Increased from 30\ntrain_dl = DataLoader(SwingDataset(train_df), batch_size=64, shuffle=True, drop_last=True)\nval_dl = DataLoader(SwingDataset(val_df), batch_size=64, shuffle=False)\n\nscheduler = OneCycleLR(\n    opt,\n    max_lr=1e-3,\n    epochs=epochs,\n    steps_per_epoch=len(train_dl),\n    pct_start=0.3,        # 30% warmup\n    anneal_strategy='cos',\n    div_factor=10.0,\n    final_div_factor=100.0\n)\n\nprint(f\"‚úì Scheduler: OneCycleLR (max_lr=1e-3, warmup=30%)\")\n\n# ========================================\n# Loss Functions\n# ========================================\n\nhuber = torch.nn.SmoothL1Loss()\nbce = torch.nn.BCEWithLogitsLoss()\n\nprint(f\"‚úì Loss: SmoothL1 (returns) + BCE (direction)\")\n\n# ========================================\n# Training Configuration\n# ========================================\n\npatience = 10         # Increased from 5\nbest_val = 1e9\nbad = 0\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Training Configuration:\")\nprint(f\"  Epochs: {epochs}\")\nprint(f\"  Batch size: 64\")\nprint(f\"  Train batches: {len(train_dl)}\")\nprint(f\"  Val batches: {len(val_dl)}\")\nprint(f\"  Early stopping patience: {patience}\")\nprint(f\"  Expected training time: ~1-2 hours on T4 GPU\")\nprint(f\"{'='*50}\\n\")\n\n# ========================================\n# Evaluation Function\n# ========================================\n\ndef evaluate():\n    \"\"\"Evaluate model on validation set\"\"\"\n    model.eval()\n    losses = []\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for x_price, x_kron, x_ctx, y_ret, y_up in val_dl:\n            x_price = x_price.to(device)\n            x_kron = x_kron.to(device)\n            x_ctx = x_ctx.to(device)\n            y_ret = y_ret.to(device)\n            y_up = y_up.to(device)\n            \n            out = model(x_price, x_kron, x_ctx)\n            \n            # Combined loss\n            loss = 0.6 * huber(out[\"ret\"], y_ret) + 0.4 * bce(out[\"up_logits\"], y_up)\n            losses.append(loss.item())\n            \n            # Track predictions for accuracy\n            preds = (torch.sigmoid(out[\"up_logits\"]) > 0.5).float()\n            all_preds.append(preds[:, 0].cpu().numpy())  # 3-day direction\n            all_labels.append(y_up[:, 0].cpu().numpy())\n    \n    model.train()\n    \n    # Calculate metrics\n    val_loss = float(np.mean(losses))\n    all_preds = np.concatenate(all_preds)\n    all_labels = np.concatenate(all_labels)\n    accuracy = (all_preds == all_labels).mean()\n    \n    return val_loss, accuracy\n\nprint(\"‚úì Ready to train!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908bd722",
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# Training Loop\n# ========================================\n\nprint(\"Starting training...\\n\")\n\ntrain_losses = []\nval_losses = []\nval_accuracies = []\n\nfor epoch in range(epochs):\n    model.train()\n    epoch_losses = []\n    \n    # Training\n    pbar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{epochs}\")\n    for x_price, x_kron, x_ctx, y_ret, y_up in pbar:\n        x_price = x_price.to(device)\n        x_kron = x_kron.to(device)\n        x_ctx = x_ctx.to(device)\n        y_ret = y_ret.to(device)\n        y_up = y_up.to(device)\n\n        # Forward pass\n        out = model(x_price, x_kron, x_ctx)\n        \n        # Combined loss\n        loss = 0.6 * huber(out[\"ret\"], y_ret) + 0.4 * bce(out[\"up_logits\"], y_up)\n        \n        # Backward pass\n        opt.zero_grad()\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        opt.step()\n        scheduler.step()\n        \n        epoch_losses.append(loss.item())\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    # Validation\n    val_loss, val_acc = evaluate()\n    train_loss = np.mean(epoch_losses)\n    \n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    val_accuracies.append(val_acc)\n    \n    print(f\"Epoch {epoch+1:2d}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n    \n    # Save best model\n    if val_loss < best_val:\n        best_val = val_loss\n        bad = 0\n        torch.save(model.state_dict(), \"artifacts/v1/stockformer/weights.pt\")\n        print(f\"  ‚úì New best model saved! (val_loss={val_loss:.4f}, val_acc={val_acc:.4f})\")\n    else:\n        bad += 1\n        if bad >= patience:\n            print(f\"\\n‚ö† Early stopping triggered after {epoch+1} epochs\")\n            break\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Training Complete!\")\nprint(f\"  Best val loss: {best_val:.4f}\")\nprint(f\"  Best val accuracy: {max(val_accuracies):.4f}\")\nprint(f\"  Total epochs: {epoch+1}\")\nprint(f\"{'='*50}\\n\")\n\n# ========================================\n# Plot Training Curves\n# ========================================\n\ntry:\n    import matplotlib.pyplot as plt\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Loss curves\n    ax1.plot(train_losses, label='Train Loss', linewidth=2)\n    ax1.plot(val_losses, label='Val Loss', linewidth=2)\n    ax1.set_xlabel('Epoch', fontsize=12)\n    ax1.set_ylabel('Loss', fontsize=12)\n    ax1.set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n    ax1.legend(fontsize=11)\n    ax1.grid(True, alpha=0.3)\n    \n    # Accuracy curve\n    ax2.plot(val_accuracies, label='Val Accuracy', linewidth=2, color='green')\n    ax2.axhline(y=0.65, color='r', linestyle='--', label='Target (65%)', alpha=0.7)\n    ax2.set_xlabel('Epoch', fontsize=12)\n    ax2.set_ylabel('Accuracy', fontsize=12)\n    ax2.set_title('Validation Accuracy', fontsize=14, fontweight='bold')\n    ax2.legend(fontsize=11)\n    ax2.grid(True, alpha=0.3)\n    ax2.set_ylim([0.4, 0.8])\n    \n    plt.tight_layout()\n    plt.savefig('artifacts/v1/stockformer/training_curves.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(\"‚úì Training curves saved to artifacts/v1/stockformer/training_curves.png\")\nexcept Exception as e:\n    print(f\"‚ö† Could not plot training curves: {e}\")\n\n# ========================================\n# Save Configuration\n# ========================================\n\ncfg = {\n  \"name\": \"timeformer_xl_v1\",\n  \"architecture\": \"TimeFormer-XL\",\n  \"lookback\": 120,\n  \"ohlcv_features\": 5,\n  \"kronos_dim\": 512,\n  \"context_dim\": 29,\n  \"horizons\": [3, 5, 10],\n  \"d_model\": 256,\n  \"n_heads\": 8,\n  \"n_layers\": 6,\n  \"ffn_dim\": 512,\n  \"patch_len\": 10,\n  \"dropout\": 0.2,\n  \"total_parameters\": total_params,\n  \"training\": {\n    \"epochs_trained\": epoch + 1,\n    \"best_val_loss\": float(best_val),\n    \"best_val_accuracy\": float(max(val_accuracies)),\n    \"optimizer\": \"AdamW\",\n    \"learning_rate\": 1e-4,\n    \"weight_decay\": 1e-5,\n    \"scheduler\": \"OneCycleLR\",\n    \"batch_size\": 64,\n    \"early_stopping_patience\": patience\n  }\n}\n\nwith open(\"artifacts/v1/stockformer/config.json\", \"w\") as f:\n    json.dump(cfg, f, indent=2)\n\nprint(\"‚úì Configuration saved to artifacts/v1/stockformer/config.json\")\nprint(\"\\nüéâ Training complete! Model ready for inference.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e2f6e",
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# Artifact Summary\n# ========================================\n\nfrom pathlib import Path\nimport json, os\n\nprint(\"=\"*60)\nprint(\"TimeFormer-XL Training Summary\")\nprint(\"=\"*60)\n\nw_path = Path('artifacts/v1/stockformer/weights.pt')\nc_path = Path('artifacts/v1/stockformer/config.json')\ncurve_path = Path('artifacts/v1/stockformer/training_curves.png')\n\nprint(f\"\\nüìÅ Artifacts directory: {w_path.parent.resolve()}\")\nprint(f\"\\n‚úì Model weights:\")\nprint(f\"    File: {w_path}\")\nprint(f\"    Exists: {w_path.exists()}\")\nif w_path.exists():\n    print(f\"    Size: {w_path.stat().st_size / 1e6:.2f} MB\")\n\nprint(f\"\\n‚úì Configuration:\")\nprint(f\"    File: {c_path}\")\nprint(f\"    Exists: {c_path.exists()}\")\n\nif c_path.exists():\n    with open(c_path) as f:\n        cfg = json.load(f)\n    \n    print(f\"\\nüìä Model Configuration:\")\n    print(f\"    Architecture: {cfg['architecture']}\")\n    print(f\"    Parameters: {cfg['total_parameters']:,}\")\n    print(f\"    d_model: {cfg['d_model']}\")\n    print(f\"    n_heads: {cfg['n_heads']}\")\n    print(f\"    n_layers: {cfg['n_layers']}\")\n    print(f\"    patch_len: {cfg['patch_len']}\")\n    \n    if 'training' in cfg:\n        print(f\"\\nüìà Training Results:\")\n        print(f\"    Epochs trained: {cfg['training']['epochs_trained']}\")\n        print(f\"    Best val loss: {cfg['training']['best_val_loss']:.4f}\")\n        print(f\"    Best val accuracy: {cfg['training']['best_val_accuracy']:.4f}\")\n        print(f\"    Optimizer: {cfg['training']['optimizer']}\")\n        print(f\"    Learning rate: {cfg['training']['learning_rate']}\")\n        \n        acc = cfg['training']['best_val_accuracy']\n        if acc >= 0.68:\n            status = \"üèÜ SOTA - Excellent!\"\n        elif acc >= 0.65:\n            status = \"‚úÖ Excellent\"\n        elif acc >= 0.60:\n            status = \"‚úÖ Good\"\n        elif acc >= 0.55:\n            status = \"‚ö†Ô∏è  Acceptable\"\n        else:\n            status = \"‚ùå Needs improvement\"\n        \n        print(f\"\\n    Status: {status}\")\n\nprint(f\"\\n‚úì Training curves:\")\nprint(f\"    File: {curve_path}\")\nprint(f\"    Exists: {curve_path.exists()}\")\n\nprint(f\"\\n{'='*60}\")\nprint(\"Next steps:\")\nprint(\"  1. Download artifacts (weights.pt, config.json)\")\nprint(\"  2. Upload to production: artifacts/v1/stockformer/\")\nprint(\"  3. Train Notebook 03 (TFT model)\")\nprint(\"  4. Train Notebook 04 (LightGBM veto)\")\nprint(\"  5. Deploy ensemble for inference\")\nprint(f\"{'='*60}\\n\")\n\nprint(\"üéâ TimeFormer-XL training complete!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}