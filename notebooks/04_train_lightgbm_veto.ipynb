{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08d53e4",
   "metadata": {},
   "source": [
    "# 04 — Train LightGBM veto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc8878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install torch numpy pandas pyarrow scikit-learn lightgbm tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, pathlib\n",
    "\n",
    "REPO_URL = os.getenv('REPO_URL', 'https://github.com/RishiKarthikeyan07/ai-trader-saas')\n",
    "REPO_DIR = os.getenv('REPO_DIR', 'AI_TRADER')\n",
    "\n",
    "if not pathlib.Path('backend').exists():\n",
    "    if not pathlib.Path(REPO_DIR).exists():\n",
    "        !git clone $REPO_URL $REPO_DIR\n",
    "    %cd $REPO_DIR\n",
    "sys.path.append(str(pathlib.Path('backend').resolve()))\n",
    "\n",
    "DATASET_PATH = pathlib.Path('training_data/v1/dataset.parquet')\n",
    "\n",
    "train_df = None\n",
    "val_df = None\n",
    "\n",
    "assert DATASET_PATH.exists(), f\"Dataset not found at {DATASET_PATH}\"\n",
    "print(f'Using dataset: {DATASET_PATH}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa03e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "\n",
    "from app.ml.preprocess.normalize import build_veto_vec\n",
    "from app.ml.stockformer.model import StockFormer\n",
    "from app.ml.tft.model import TFT\n",
    "\n",
    "DATASET_PATH = pathlib.Path('training_data/v1/dataset.parquet')\n",
    "df = pd.read_parquet(DATASET_PATH).sort_values('asof').reset_index(drop=True)\n",
    "split = int(0.8 * len(df))\n",
    "train_df, val_df = df.iloc[:split], df.iloc[split:]\n",
    "print(f'Train: {len(train_df)}, Val: {len(val_df)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "t8scuxecd4",
   "source": "# ========================================\n# Set Random Seeds for Reproducibility\n# ========================================\n\nimport random\nimport numpy as np\n\nSEED = 42\n\n# Set seeds\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\nprint(f\"✓ Random seeds set to {SEED} for reproducibility\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e15f8d",
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# Load Trained Models with Correct Configs\n# ========================================\n\nimport os\n\n# Check if model artifacts exist\nsf_weights_path = 'artifacts/v1/stockformer/weights.pt'\nsf_config_path = 'artifacts/v1/stockformer/config.json'\ntft_weights_path = 'artifacts/v1/tft/weights.pt'\ntft_config_path = 'artifacts/v1/tft/config.json'\n\nif not os.path.exists(sf_weights_path):\n    raise FileNotFoundError(f\"StockFormer weights not found at {sf_weights_path}. Train Notebook 02 first!\")\nif not os.path.exists(sf_config_path):\n    raise FileNotFoundError(f\"StockFormer config not found at {sf_config_path}. Train Notebook 02 first!\")\nif not os.path.exists(tft_weights_path):\n    raise FileNotFoundError(f\"TFT weights not found at {tft_weights_path}. Train Notebook 03 first!\")\nif not os.path.exists(tft_config_path):\n    raise FileNotFoundError(f\"TFT config not found at {tft_config_path}. Train Notebook 03 first!\")\n\nprint(\"✓ All artifact files found\")\n\n# Load StockFormer config\nwith open(sf_config_path) as f:\n    sf_cfg = json.load(f)\n\nprint(f\"\\nStockFormer config:\")\nprint(f\"  d_model: {sf_cfg.get('d_model', 'N/A')}\")\nprint(f\"  n_heads: {sf_cfg.get('n_heads', 'N/A')}\")\nprint(f\"  n_layers: {sf_cfg.get('n_layers', 'N/A')}\")\nprint(f\"  ffn_dim: {sf_cfg.get('ffn_dim', 'N/A')}\")\n\n# Create StockFormer with trained config\nsf = StockFormer(\n    lookback=sf_cfg.get('lookback', 120),\n    price_dim=sf_cfg.get('price_dim', 5),\n    kronos_dim=sf_cfg.get('kronos_dim', 512),\n    context_dim=sf_cfg.get('context_dim', 29),\n    d_model=sf_cfg.get('d_model', 256),       # SOTA: 256\n    n_heads=sf_cfg.get('n_heads', 8),         # SOTA: 8\n    n_layers=sf_cfg.get('n_layers', 6),       # SOTA: 6\n    ffn_dim=sf_cfg.get('ffn_dim', 512),       # SOTA: 512\n    patch_len=sf_cfg.get('patch_len', 10),\n    dropout=sf_cfg.get('dropout', 0.2),\n    num_horizons=sf_cfg.get('num_horizons', 3)\n)\n\n# Load weights\nsf.load_state_dict(torch.load(sf_weights_path, map_location='cpu'))\nsf.eval()\nprint(\"✓ StockFormer loaded\")\n\n# Load TFT config\nwith open(tft_config_path) as f:\n    tft_cfg = json.load(f)\n\nprint(f\"\\nTFT config:\")\nprint(f\"  emb_dim: {tft_cfg.get('emb_dim', 'N/A')}\")\nprint(f\"  hidden_size: {tft_cfg.get('hidden_size', 'N/A')}\")\nprint(f\"  n_heads: {tft_cfg.get('n_heads', 'N/A')}\")\nprint(f\"  num_layers: {tft_cfg.get('num_layers', 'N/A')}\")\n\n# Create TFT with trained config\ntft = TFT(\n    lookback=tft_cfg.get('lookback', 120),\n    price_dim=tft_cfg.get('price_dim', 5),\n    kronos_dim=tft_cfg.get('kronos_dim', 512),\n    context_dim=tft_cfg.get('context_dim', 29),\n    emb_dim=tft_cfg.get('emb_dim', 128),          # SOTA: 128\n    hidden_size=tft_cfg.get('hidden_size', 256),  # SOTA: 256\n    n_heads=tft_cfg.get('n_heads', 8),            # SOTA: 8\n    num_layers=tft_cfg.get('num_layers', 3),      # SOTA: 3\n    dropout=tft_cfg.get('dropout', 0.1),\n    num_horizons=tft_cfg.get('num_horizons', 3)\n)\n\n# Load weights\ntft.load_state_dict(torch.load(tft_weights_path, map_location='cpu'))\ntft.eval()\nprint(\"✓ TFT loaded\")\n\nprint(f\"\\n{'='*50}\")\nprint(\"Models loaded successfully with correct configurations!\")\nprint(f\"{'='*50}\\n\")\n\ndef split_ctx(ctx: np.ndarray):\n    \"\"\"Split context vector into MTF, SMC, TA components\"\"\"\n    a = np.asarray(ctx, dtype=np.float32).reshape(-1)\n    tf_align = a[:5]     # MTF alignment (5D)\n    smc_vec = a[5:17]    # SMC features (12D)\n    ta_vec = a[17:]      # TA indicators (12D)\n    return tf_align, smc_vec, ta_vec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb063f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_vectors(frame: pd.DataFrame, max_rows: int | None = None):\n",
    "    X, y = [], []\n",
    "    iterator = frame if max_rows is None else frame.sample(min(max_rows, len(frame)), random_state=42)\n",
    "\n",
    "    def _arr_price(x, idx=None):\n",
    "        try:\n",
    "            a = np.array(x, dtype=np.float32)\n",
    "        except Exception:\n",
    "            a = np.array(list(x), dtype=object)\n",
    "            a = np.stack([np.array(row, dtype=np.float32).reshape(-1) for row in a], axis=0)\n",
    "        if a.size != 120 * 5:\n",
    "            raise ValueError(f\"Bad ohlcv_norm size for idx {idx}: shape {a.shape}\")\n",
    "        return a.reshape(1, 120, 5)\n",
    "\n",
    "    def _arr_flat(x, name, idx=None):\n",
    "        try:\n",
    "            a = np.array(x, dtype=np.float32).reshape(1, -1)\n",
    "        except Exception as exc:\n",
    "            raise ValueError(f\"Bad array for {name} at idx {idx}: {x}\") from exc\n",
    "        return a\n",
    "\n",
    "    for i, (_, r) in enumerate(iterator.iterrows()):\n",
    "        x_price = torch.tensor(_arr_price(r['ohlcv_norm'], idx=i))\n",
    "        x_kron = torch.tensor(_arr_flat(r['kronos_emb'], 'kronos_emb', idx=i))\n",
    "        x_ctx = torch.tensor(_arr_flat(r['context'], 'context', idx=i))\n",
    "        with torch.no_grad():\n",
    "            sf_out = sf(x_price, x_kron, x_ctx)\n",
    "            tft_out = tft(x_price, x_kron, x_ctx)\n",
    "        tf_align, smc_vec, ta_vec = split_ctx(r['context'])\n",
    "        veto_vec = build_veto_vec(\n",
    "            sf_out={'prob': sf_out['up_prob'].numpy(), 'ret': sf_out['ret'].numpy()},\n",
    "            tft_out={k: v.numpy() for k, v in tft_out.items()},\n",
    "            smc_vec=smc_vec,\n",
    "            tf_align=tf_align,\n",
    "            ta_vec=ta_vec,\n",
    "            raw_features={}\n",
    "        )\n",
    "        X.append(veto_vec.squeeze(0))\n",
    "        y.append(int(np.array(r['y_up'])[1]))\n",
    "    return np.stack(X, axis=0), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = infer_vectors(train_df, max_rows=50000)\n",
    "X_val, y_val = infer_vectors(val_df, max_rows=10000)\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective='binary',\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "lgbm.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='logloss',\n",
    "    verbose=50,\n",
    ")\n",
    "\n",
    "booster = lgbm.booster_\n",
    "booster.save_model('artifacts/v1/veto/lightgbm.txt')\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "Path('artifacts/v1/veto').mkdir(parents=True, exist_ok=True)\n",
    "with open('artifacts/v1/veto/config.json','w') as f:\n",
    "    json.dump({'name':'lightgbm_veto_v1','threshold_block':0.65,'threshold_boost':0.35}, f, indent=2)\n",
    "print('Saved LightGBM veto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7c9952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifact summary for LightGBM veto\n",
    "from pathlib import Path\n",
    "import json, os\n",
    "\n",
    "w_path = Path('artifacts/v1/veto/lightgbm.txt')\n",
    "c_path = Path('artifacts/v1/veto/config.json')\n",
    "print('Artifacts directory exists:', w_path.parent.exists())\n",
    "print('LightGBM model exists:', w_path.exists(), w_path)\n",
    "print('Config exists:', c_path.exists(), c_path)\n",
    "if c_path.exists():\n",
    "    print(c_path.read_text())\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}