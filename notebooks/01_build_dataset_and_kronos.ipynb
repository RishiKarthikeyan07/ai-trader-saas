{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 \u2014 Build training dataset + Kronos embeddings (512d)\n",
    "\n",
    "Runs on Colab T4. Pulls yfinance daily bars, builds TF-align/SMC/TA vectors via in-repo preprocessors, encodes Kronos embeddings, and saves `training_data/v1/dataset.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install yfinance pandas numpy pyarrow duckdb torch huggingface_hub tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'AI_TRADER'...\n",
      "fatal: could not read Username for 'https://github.com': No such device or address\n",
      "[Errno 2] No such file or directory: 'AI_TRADER'\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, pathlib\n",
    "\n",
    "# If running in Colab, clone the repo first. Set REPO_URL env if needed.\n",
    "REPO_URL = os.getenv(\"REPO_URL\", \"https://github.com/RishiKarthikeyan07/ai-trader-saas\")\n",
    "REPO_DIR = os.getenv(\"REPO_DIR\", \"AI_TRADER\")\n",
    "if not pathlib.Path(\"backend\").exists():\n",
    "    if not pathlib.Path(REPO_DIR).exists():\n",
    "        !git clone $REPO_URL $REPO_DIR\n",
    "    %cd $REPO_DIR\n",
    "\n",
    "sys.path.append(os.path.abspath(\"backend\"))\n",
    "os.makedirs(\"training_data/v1\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'app'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2937909665.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m from app.ml.preprocess.normalize import (\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mnormalize_ohlcv_120\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbuild_tf_align_vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'app'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from app.ml.preprocess.normalize import (\n",
    "    normalize_ohlcv_120,\n",
    "    build_tf_align_vec,\n",
    "    build_smc_vec,\n",
    "    build_ta_vec,\n",
    ")\n",
    "from app.services.kronos_loader import load_kronos_hf\n",
    "from app.services.feature_engine import compute_ta_features, compute_smc_features\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "LOOKBACK = 120\n",
    "HORIZONS = [3, 5, 10]\n",
    "START = os.getenv(\"DATA_START\", \"2015-01-01\")  # 9y+ history for NIFTY500\n",
    "END = os.getenv(\"DATA_END\", None)  # None == today\n",
    "TICKER_FILE = os.getenv(\"TICKER_FILE\", \"training_data/nifty500_symbols.txt\")\n",
    "\n",
    "if Path(TICKER_FILE).exists():\n",
    "    with open(TICKER_FILE) as f:\n",
    "        TICKERS = [t.strip() for t in f if t.strip()]\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Ticker file {TICKER_FILE} not found. Add NIFTY500 symbols (e.g., RELIANCE.NS) one per line or set TICKER_FILE env.\"\n",
    "    )\n",
    "\n",
    "OUT_PATH = pathlib.Path(\"training_data/v1/dataset.parquet\")\n",
    "print(f\"Using {len(TICKERS)} tickers; saving to {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_daily(sym: str) -> pd.DataFrame:\n",
    "    df = yf.download(sym, start=START, end=END, interval=\"1d\", auto_adjust=False, progress=False)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.rename(columns=str.lower)[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].dropna()\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={\"index\": \"date\", \"Date\": \"date\"}, inplace=True)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df.set_index(\"date\", inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for h in HORIZONS:\n",
    "        out[f\"ret_{h}\"] = (out[\"close\"].shift(-h) / out[\"close\"]) - 1.0\n",
    "        out[f\"up_{h}\"] = (out[f\"ret_{h}\"] > 0).astype(np.int32)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction helpers aligned with backend\n",
    "def _prep_window(window: pd.DataFrame) -> pd.DataFrame | None:\n",
    "    if window.empty:\n",
    "        return None\n",
    "    df = window.copy()\n",
    "    df.columns = [str(c).lower() for c in df.columns]\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "    df = df.sort_index()\n",
    "    required = ['open', 'high', 'low', 'close', 'volume']\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_alignment(window: pd.DataFrame) -> dict:\n",
    "    base = _prep_window(window)\n",
    "    if base is None:\n",
    "        return {'monthly_bias': 0.0, 'weekly_bias': 0.0, 'daily_bias': 0.0, 'h4_align': 0.0, 'h1_align': 0.0}\n",
    "    core = base[['open', 'high', 'low', 'close', 'volume']]\n",
    "    wk = core.resample('W-FRI').agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last', 'volume': 'sum'}).dropna()\n",
    "    mo = core.resample('ME').agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last', 'volume': 'sum'}).dropna()\n",
    "    h4 = core.resample('4h').agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last', 'volume': 'sum'}).dropna()\n",
    "    h1 = core.copy()  # already 1H if provided; with daily data it's sparse but harmless\n",
    "\n",
    "    def bias(df: pd.DataFrame) -> float:\n",
    "        enriched = compute_ta_features(df)\n",
    "        if enriched.empty:\n",
    "            return 0.0\n",
    "        latest = enriched.iloc[-1]\n",
    "        return 1.0 if latest.get('ema_fast', 0) > latest.get('ema_slow', 0) else -1.0\n",
    "\n",
    "    return {\n",
    "        'monthly_bias': bias(mo),\n",
    "        'weekly_bias': bias(wk),\n",
    "        'daily_bias': bias(core),\n",
    "        'h4_align': bias(h4),\n",
    "        'h1_align': bias(h1),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_feature_dict(window: pd.DataFrame) -> dict:\n",
    "    base = _prep_window(window)\n",
    "    if base is None:\n",
    "        return {}\n",
    "    enriched = compute_ta_features(base)\n",
    "    enriched = compute_smc_features(enriched)\n",
    "    if enriched.empty:\n",
    "        return {}\n",
    "    latest = enriched.iloc[-1].to_dict()\n",
    "    latest.update({\n",
    "        'open': float(base.iloc[-1]['open']),\n",
    "        'high': float(base.iloc[-1]['high']),\n",
    "        'low': float(base.iloc[-1]['low']),\n",
    "        'close': float(base.iloc[-1]['close']),\n",
    "        'volume': float(base.iloc[-1]['volume']),\n",
    "    })\n",
    "    return latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kronos 512d encoder\n",
    "kronos = load_kronos_hf(device=device, max_context=512)\n",
    "\n",
    "def kronos_embed(batch_norm: np.ndarray) -> np.ndarray:\n",
    "    # batch_norm: (B,120,5)\n",
    "    x = torch.tensor(batch_norm, dtype=torch.float32, device=device)\n",
    "    if x.shape[-1] == 5:  # pad amount channel if tokenizer expects 6\n",
    "        amt = torch.zeros(x.shape[0], x.shape[1], 1, device=device)\n",
    "        x = torch.cat([x, amt], dim=-1)\n",
    "    z = kronos.tokenizer.embed(x)\n",
    "    if isinstance(z, tuple):\n",
    "        z = z[0]\n",
    "    emb = z.mean(dim=1).detach().cpu().numpy().astype(np.float32)\n",
    "    if emb.shape[1] < 512:\n",
    "        pad = np.zeros((emb.shape[0], 512 - emb.shape[1]), dtype=np.float32)\n",
    "        emb = np.concatenate([emb, pad], axis=1)\n",
    "    elif emb.shape[1] > 512:\n",
    "        emb = emb[:, :512]\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for sym in tqdm(TICKERS):\n",
    "    df = fetch_daily(sym)\n",
    "    if df.empty or len(df) < LOOKBACK + max(HORIZONS) + 10:\n",
    "        continue\n",
    "    df = add_labels(df)\n",
    "\n",
    "    batch_ohlcv = []\n",
    "    batch_meta = []\n",
    "\n",
    "    for i in range(LOOKBACK - 1, len(df) - max(HORIZONS)):\n",
    "        window = df.iloc[i - LOOKBACK + 1 : i + 1]\n",
    "        ohlcv = window[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].values.astype(np.float32)\n",
    "        if ohlcv.shape[0] != LOOKBACK:\n",
    "            continue\n",
    "        norm = normalize_ohlcv_120(ohlcv)\n",
    "        alignment = build_tf_align_vec(compute_alignment(window))\n",
    "        feat_dict = compute_feature_dict(window)\n",
    "        smc_vec = build_smc_vec(feat_dict)\n",
    "        ta_vec = build_ta_vec(feat_dict)\n",
    "        context = np.concatenate([alignment, smc_vec, ta_vec]).astype(np.float32)\n",
    "\n",
    "        y_ret = np.array([df.iloc[i][f\"ret_{h}\"] for h in HORIZONS], dtype=np.float32)\n",
    "        y_up = np.array([df.iloc[i][f\"up_{h}\"] for h in HORIZONS], dtype=np.float32)\n",
    "        if np.any(np.isnan(y_ret)):\n",
    "            continue\n",
    "\n",
    "        batch_ohlcv.append(norm)\n",
    "        batch_meta.append((df.index[i], context, y_ret, y_up))\n",
    "\n",
    "        if len(batch_ohlcv) >= 64:\n",
    "            emb = kronos_embed(np.stack(batch_ohlcv, axis=0))\n",
    "            for (asof, ctx, y_r, y_u), e, o in zip(batch_meta, emb, batch_ohlcv):\n",
    "                rows.append({\n",
    "                    \"symbol\": sym,\n",
    "                    \"asof\": pd.to_datetime(asof),\n",
    "                    \"ohlcv_norm\": o,\n",
    "                    \"kronos_emb\": e,\n",
    "                    \"context\": ctx,\n",
    "                    \"y_ret\": y_r,\n",
    "                    \"y_up\": y_u,\n",
    "                })\n",
    "            batch_ohlcv, batch_meta = [], []\n",
    "\n",
    "    if batch_ohlcv:\n",
    "        emb = kronos_embed(np.stack(batch_ohlcv, axis=0))\n",
    "        for (asof, ctx, y_r, y_u), e, o in zip(batch_meta, emb, batch_ohlcv):\n",
    "            rows.append({\n",
    "                \"symbol\": sym,\n",
    "                \"asof\": pd.to_datetime(asof),\n",
    "                \"ohlcv_norm\": o,\n",
    "                \"kronos_emb\": e,\n",
    "                \"context\": ctx,\n",
    "                \"y_ret\": y_r,\n",
    "                \"y_up\": y_u,\n",
    "            })\n",
    "\n",
    "print(f\"Total samples: {len(rows)}\")\n",
    "df_out = pd.DataFrame(rows)\n",
    "# Convert array columns to lists for Parquet compatibility\n",
    "for _col in ['ohlcv_norm','kronos_emb','context','y_ret','y_up']:\n",
    "    df_out[_col] = df_out[_col].apply(lambda x: x.tolist())\n",
    "df_out.to_parquet(OUT_PATH, index=False)\n",
    "print(f\"Saved to {OUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}