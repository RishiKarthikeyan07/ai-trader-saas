{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 â€” Build training dataset + Kronos embeddings (512d)\n",
    "\n",
    "Runs on Colab T4. Pulls yfinance daily bars, builds TF-align/SMC/TA vectors via in-repo preprocessors, encodes Kronos embeddings, and saves `training_data/v1/dataset.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install yfinance pandas numpy pyarrow duckdb torch huggingface_hub tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, pathlib\n",
    "\n",
    "# If running in Colab, clone the repo first. Set REPO_URL env if needed.\n",
    "REPO_URL = os.getenv(\"REPO_URL\", \"https://github.com/your-org/AI_TRADER.git\")\n",
    "REPO_DIR = os.getenv(\"REPO_DIR\", \"AI_TRADER\")\n",
    "if not pathlib.Path(\"backend\").exists():\n",
    "    if not pathlib.Path(REPO_DIR).exists():\n",
    "        !git clone $REPO_URL $REPO_DIR\n",
    "    %cd $REPO_DIR\n",
    "\n",
    "sys.path.append(os.path.abspath(\"backend\"))\n",
    "os.makedirs(\"training_data/v1\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from app.ml.preprocess.normalize import (\n",
    "    normalize_ohlcv_120,\n",
    "    build_tf_align_vec,\n",
    "    build_smc_vec,\n",
    "    build_ta_vec,\n",
    ")\n",
    "from app.services.kronos_loader import load_kronos_hf\n",
    "from app.services.feature_engine import compute_ta_features, compute_smc_features\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "LOOKBACK = 120\n",
    "HORIZONS = [3, 5, 10]\n",
    "START = os.getenv(\"DATA_START\", \"2020-01-01\")\n",
    "END = os.getenv(\"DATA_END\", None)  # None == today\n",
    "TICKER_FILE = os.getenv(\"TICKER_FILE\", \"training_data/nifty200_symbols.txt\")\n",
    "DEFAULT_TICKERS = [\"RELIANCE.NS\", \"TCS.NS\", \"HDFCBANK.NS\", \"INFY.NS\", \"ICICIBANK.NS\"]\n",
    "\n",
    "if pathlib.Path(TICKER_FILE).exists():\n",
    "    with open(TICKER_FILE) as f:\n",
    "        TICKERS = [t.strip() for t in f if t.strip()]\n",
    "else:\n",
    "    TICKERS = DEFAULT_TICKERS\n",
    "\n",
    "OUT_PATH = pathlib.Path(\"training_data/v1/dataset.parquet\")\n",
    "print(f\"Using {len(TICKERS)} tickers; saving to {OUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_daily(sym: str) -> pd.DataFrame:\n",
    "    df = yf.download(sym, start=START, end=END, interval=\"1d\", auto_adjust=False, progress=False)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.rename(columns=str.lower)[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].dropna()\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={\"index\": \"date\", \"Date\": \"date\"}, inplace=True)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df.set_index(\"date\", inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for h in HORIZONS:\n",
    "        out[f\"ret_{h}\"] = (out[\"close\"].shift(-h) / out[\"close\"]) - 1.0\n",
    "        out[f\"up_{h}\"] = (out[f\"ret_{h}\"] > 0).astype(np.int32)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction helpers aligned with backend\n",
    "def compute_alignment(window: pd.DataFrame) -> dict:\n",
    "    if window.empty:\n",
    "        return {\"monthly_bias\": 0.0, \"weekly_bias\": 0.0, \"daily_bias\": 0.0, \"h4_align\": 0.0, \"h1_align\": 0.0}\n",
    "    # Resample from the available daily window\n",
    "    wk = window.resample(\"W-FRI\").agg({\"open\": \"first\", \"high\": \"max\", \"low\": \"min\", \"close\": \"last\", \"volume\": \"sum\"}).dropna()\n",
    "    mo = window.resample(\"ME\").agg({\"open\": \"first\", \"high\": \"max\", \"low\": \"min\", \"close\": \"last\", \"volume\": \"sum\"}).dropna()\n",
    "    h4 = window.resample(\"4h\").agg({\"open\": \"first\", \"high\": \"max\", \"low\": \"min\", \"close\": \"last\", \"volume\": \"sum\"}).dropna()\n",
    "    h1 = window.copy()  # already 1H if provided; with daily data it's sparse but harmless\n",
    "\n",
    "    def bias(df: pd.DataFrame) -> float:\n",
    "        enriched = compute_ta_features(df)\n",
    "        if enriched.empty:\n",
    "            return 0.0\n",
    "        latest = enriched.iloc[-1]\n",
    "        return 1.0 if latest.get(\"ema_fast\", 0) > latest.get(\"ema_slow\", 0) else -1.0\n",
    "\n",
    "    return {\n",
    "        \"monthly_bias\": bias(mo),\n",
    "        \"weekly_bias\": bias(wk),\n",
    "        \"daily_bias\": bias(window),\n",
    "        \"h4_align\": bias(h4),\n",
    "        \"h1_align\": bias(h1),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_feature_dict(window: pd.DataFrame) -> dict:\n",
    "    enriched = compute_ta_features(window)\n",
    "    enriched = compute_smc_features(enriched)\n",
    "    if enriched.empty:\n",
    "        return {}\n",
    "    latest = enriched.iloc[-1].to_dict()\n",
    "    # ensure raw OHLCV for TA vec builders\n",
    "    latest.update({\n",
    "        \"open\": float(window.iloc[-1][\"open\"]),\n",
    "        \"high\": float(window.iloc[-1][\"high\"]),\n",
    "        \"low\": float(window.iloc[-1][\"low\"]),\n",
    "        \"close\": float(window.iloc[-1][\"close\"]),\n",
    "        \"volume\": float(window.iloc[-1][\"volume\"]),\n",
    "    })\n",
    "    return latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kronos 512d encoder\n",
    "kronos = load_kronos_hf(device=device, max_context=512)\n",
    "\n",
    "def kronos_embed(batch_norm: np.ndarray) -> np.ndarray:\n",
    "    # batch_norm: (B,120,5)\n",
    "    x = torch.tensor(batch_norm, dtype=torch.float32, device=device)\n",
    "    if x.shape[-1] == 5:  # pad amount channel if tokenizer expects 6\n",
    "        amt = torch.zeros(x.shape[0], x.shape[1], 1, device=device)\n",
    "        x = torch.cat([x, amt], dim=-1)\n",
    "    z = kronos.tokenizer.embed(x)\n",
    "    if isinstance(z, tuple):\n",
    "        z = z[0]\n",
    "    emb = z.mean(dim=1).detach().cpu().numpy().astype(np.float32)\n",
    "    if emb.shape[1] < 512:\n",
    "        pad = np.zeros((emb.shape[0], 512 - emb.shape[1]), dtype=np.float32)\n",
    "        emb = np.concatenate([emb, pad], axis=1)\n",
    "    elif emb.shape[1] > 512:\n",
    "        emb = emb[:, :512]\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for sym in tqdm(TICKERS):\n",
    "    df = fetch_daily(sym)\n",
    "    if df.empty or len(df) < LOOKBACK + max(HORIZONS) + 10:\n",
    "        continue\n",
    "    df = add_labels(df)\n",
    "\n",
    "    batch_ohlcv = []\n",
    "    batch_meta = []\n",
    "\n",
    "    for i in range(LOOKBACK - 1, len(df) - max(HORIZONS)):\n",
    "        window = df.iloc[i - LOOKBACK + 1 : i + 1]\n",
    "        ohlcv = window[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].values.astype(np.float32)\n",
    "        if ohlcv.shape[0] != LOOKBACK:\n",
    "            continue\n",
    "        norm = normalize_ohlcv_120(ohlcv)\n",
    "        alignment = build_tf_align_vec(compute_alignment(window))\n",
    "        feat_dict = compute_feature_dict(window)\n",
    "        smc_vec = build_smc_vec(feat_dict)\n",
    "        ta_vec = build_ta_vec(feat_dict)\n",
    "        context = np.concatenate([alignment, smc_vec, ta_vec]).astype(np.float32)\n",
    "\n",
    "        y_ret = np.array([df.iloc[i][f\"ret_{h}\"] for h in HORIZONS], dtype=np.float32)\n",
    "        y_up = np.array([df.iloc[i][f\"up_{h}\"] for h in HORIZONS], dtype=np.float32)\n",
    "        if np.any(np.isnan(y_ret)):\n",
    "            continue\n",
    "\n",
    "        batch_ohlcv.append(norm)\n",
    "        batch_meta.append((df.index[i], context, y_ret, y_up))\n",
    "\n",
    "        if len(batch_ohlcv) >= 64:\n",
    "            emb = kronos_embed(np.stack(batch_ohlcv, axis=0))\n",
    "            for (asof, ctx, y_r, y_u), e, o in zip(batch_meta, emb, batch_ohlcv):\n",
    "                rows.append({\n",
    "                    \"symbol\": sym,\n",
    "                    \"asof\": pd.to_datetime(asof),\n",
    "                    \"ohlcv_norm\": o,\n",
    "                    \"kronos_emb\": e,\n",
    "                    \"context\": ctx,\n",
    "                    \"y_ret\": y_r,\n",
    "                    \"y_up\": y_u,\n",
    "                })\n",
    "            batch_ohlcv, batch_meta = [], []\n",
    "\n",
    "    if batch_ohlcv:\n",
    "        emb = kronos_embed(np.stack(batch_ohlcv, axis=0))\n",
    "        for (asof, ctx, y_r, y_u), e, o in zip(batch_meta, emb, batch_ohlcv):\n",
    "            rows.append({\n",
    "                \"symbol\": sym,\n",
    "                \"asof\": pd.to_datetime(asof),\n",
    "                \"ohlcv_norm\": o,\n",
    "                \"kronos_emb\": e,\n",
    "                \"context\": ctx,\n",
    "                \"y_ret\": y_r,\n",
    "                \"y_up\": y_u,\n",
    "            })\n",
    "\n",
    "print(f\"Total samples: {len(rows)}\")\n",
    "df_out = pd.DataFrame(rows)\n",
    "df_out.to_parquet(OUT_PATH, index=False)\n",
    "print(f\"Saved to {OUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
