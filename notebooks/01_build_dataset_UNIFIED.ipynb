{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 — Build training dataset using Unified Feature Pipeline\n",
    "\n",
    "**NEW VERSION:** Uses `UnifiedFeaturePipeline` to ensure training and inference use IDENTICAL features.\n",
    "\n",
    "Features:\n",
    "- OHLCV normalization: (120, 5) → [-1, 1]\n",
    "- Kronos embeddings: (512,) time series foundation model\n",
    "- Context vector: (29,) = 5 MTF + 12 SMC + 12 TA\n",
    "\n",
    "Runs on Colab T4 or local GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip -q install yfinance pandas numpy pyarrow duckdb torch huggingface_hub tqdm\n",
    "!pip -q install scikit-learn lightgbm joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, sys, pathlib\n\n# Get current working directory\ncwd = pathlib.Path().resolve()\n\n# Detect repo root\n# If running from notebooks/, go up one level\nif cwd.name == \"notebooks\":\n    repo_root = cwd.parent\n# If running from repo root\nelif (cwd / \"apps\").exists() or (cwd / \"notebooks\").exists():\n    repo_root = cwd\n# If running from somewhere else, search upwards\nelse:\n    repo_root = None\n    for p in [cwd, *cwd.parents]:\n        if (p / \"apps\").exists() and (p / \"notebooks\").exists():\n            repo_root = p\n            break\n    \n    if repo_root is None:\n        raise FileNotFoundError(\n            f\"Cannot find AI_TRADER repo root. Current directory: {cwd}\\n\"\n            \"Please run this notebook from the repo root or notebooks/ folder.\"\n        )\n\n# Change to repo root\nos.chdir(repo_root)\nprint(f\"✓ Repo root: {repo_root}\")\n\n# Add apps/api to Python path\napi_path = repo_root / \"apps\" / \"api\"\nif api_path.exists():\n    sys.path.insert(0, str(api_path))\n    print(f\"✓ Added to path: {api_path}\")\nelse:\n    raise FileNotFoundError(\n        f\"❌ apps/api not found at {api_path}\\n\"\n        \"Make sure you're running from the correct repository.\"\n    )\n\n# Create output directory\nos.makedirs(repo_root / \"training_data/v1\", exist_ok=True)\nprint(f\"✓ Output directory ready: {repo_root / 'training_data/v1'}\")\n\n# Verify we can import\ntry:\n    import app\n    print(f\"✓ Can import 'app' module from: {app.__file__}\")\nexcept ImportError as e:\n    print(f\"⚠️ Warning: Cannot import 'app' module: {e}\")\n    print(f\"   Python path: {sys.path[0]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Import Unified Feature Pipeline\n",
    "from app.ml.unified_features import UnifiedFeaturePipeline\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\n✓ Using device: {device}\")\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "LOOKBACK = 120\n",
    "HORIZONS = [3, 5, 10]\n",
    "START = os.getenv(\"DATA_START\", \"2015-01-01\")\n",
    "END = os.getenv(\"DATA_END\", None)\n",
    "TICKER_FILE = Path(os.getenv(\"TICKER_FILE\", repo_root / \"config/nifty100_yfinance.txt\"))\n",
    "\n",
    "# Load tickers\n",
    "if TICKER_FILE.exists():\n",
    "    with open(TICKER_FILE) as f:\n",
    "        TICKERS = [t.strip() for t in f if t.strip()]\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Ticker file {TICKER_FILE} not found\")\n",
    "\n",
    "if not TICKERS:\n",
    "    raise ValueError(\"No tickers loaded\")\n",
    "\n",
    "OUT_PATH = repo_root / \"training_data/v1/dataset.parquet\"\n",
    "print(f\"Using {len(TICKERS)} tickers\")\n",
    "print(f\"Saving to: {OUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Unified Feature Pipeline\n",
    "print(\"Initializing Unified Feature Pipeline...\")\n",
    "pipeline = UnifiedFeaturePipeline(\n",
    "    device=device,\n",
    "    lookback=LOOKBACK,\n",
    "    enable_kronos=True\n",
    ")\n",
    "print(\"✓ Pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data fetching functions\n",
    "ALPHA_KEY = os.getenv('ALPHAVANTAGE_API_KEY')\n",
    "\n",
    "def fetch_daily(sym: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch daily OHLCV data (AlphaVantage fallback to yfinance)\"\"\"\n",
    "    bases = []\n",
    "    if sym.endswith('.NS') or sym.endswith('.BSE'):\n",
    "        base = sym.split('.')[0]\n",
    "    else:\n",
    "        base = sym\n",
    "    bases = [f'NSE:{base}', f'BSE:{base}', base]\n",
    "\n",
    "    # Try AlphaVantage first\n",
    "    if ALPHA_KEY:\n",
    "        for av_sym in bases:\n",
    "            for attempt in range(4):\n",
    "                try:\n",
    "                    params = {\n",
    "                        'function': 'TIME_SERIES_DAILY_ADJUSTED',\n",
    "                        'symbol': av_sym,\n",
    "                        'outputsize': 'full',\n",
    "                        'apikey': ALPHA_KEY,\n",
    "                    }\n",
    "                    resp = requests.get('https://www.alphavantage.co/query', params=params, timeout=30)\n",
    "                    data = resp.json()\n",
    "                    if 'Note' in data:\n",
    "                        import time\n",
    "                        time.sleep(15)\n",
    "                        continue\n",
    "                    series = data.get('Time Series (Daily)', {})\n",
    "                    if series:\n",
    "                        records = []\n",
    "                        for date, vals in series.items():\n",
    "                            records.append({\n",
    "                                'date': pd.to_datetime(date),\n",
    "                                'open': float(vals['1. open']),\n",
    "                                'high': float(vals['2. high']),\n",
    "                                'low': float(vals['3. low']),\n",
    "                                'close': float(vals['4. close']),\n",
    "                                'volume': float(vals['6. volume']),\n",
    "                            })\n",
    "                        df = pd.DataFrame(records).sort_values('date')\n",
    "                        df.set_index('date', inplace=True)\n",
    "                        if END:\n",
    "                            df = df.loc[(df.index >= pd.to_datetime(START)) & (df.index <= pd.to_datetime(END))]\n",
    "                        else:\n",
    "                            df = df.loc[df.index >= pd.to_datetime(START)]\n",
    "                        if not df.empty:\n",
    "                            return df\n",
    "                except Exception as exc:\n",
    "                    print(f'[warn] AlphaVantage failed for {av_sym} attempt {attempt+1}: {exc}')\n",
    "                import time\n",
    "                time.sleep(15)\n",
    "\n",
    "    # Fallback to yfinance\n",
    "    last_exc = None\n",
    "    for attempt in range(6):\n",
    "        try:\n",
    "            df = yf.download(sym, start=START, end=END, interval='1d',\n",
    "                           auto_adjust=False, progress=False, threads=False)\n",
    "            if not df.empty:\n",
    "                break\n",
    "        except Exception as exc:\n",
    "            last_exc = exc\n",
    "        import time\n",
    "        time.sleep(2.0 * (attempt + 1))\n",
    "    else:\n",
    "        if last_exc:\n",
    "            print(f'[warn] {sym} failed: {last_exc}')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = df.rename(columns=str.lower)[['open', 'high', 'low', 'close', 'volume']].dropna()\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index': 'date', 'Date': 'date'}, inplace=True)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add forward return labels\"\"\"\n",
    "    out = df.copy()\n",
    "    for h in HORIZONS:\n",
    "        out[f'ret_{h}'] = (out['close'].shift(-h) / out['close']) - 1.0\n",
    "        out[f'up_{h}'] = (out[f'ret_{h}'] > 0).astype(np.int32)\n",
    "    return out\n",
    "\n",
    "print(\"✓ Data fetching functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset using Unified Feature Pipeline\n",
    "import time\n",
    "\n",
    "rows = []\n",
    "\n",
    "for sym in tqdm(TICKERS, desc=\"Processing symbols\"):\n",
    "    time.sleep(2.0)  # Throttle API calls\n",
    "    \n",
    "    df = fetch_daily(sym)\n",
    "    if df.empty or len(df) < LOOKBACK + max(HORIZONS) + 10:\n",
    "        continue\n",
    "    \n",
    "    df = add_labels(df)\n",
    "    \n",
    "    # Use unified pipeline to compute features for all windows\n",
    "    try:\n",
    "        # Get all windows\n",
    "        for i in range(LOOKBACK - 1, len(df) - max(HORIZONS)):\n",
    "            window_df = df.iloc[:i+1]  # All data up to this point\n",
    "            \n",
    "            # Skip if not enough data\n",
    "            if len(window_df) < LOOKBACK:\n",
    "                continue\n",
    "            \n",
    "            # Compute features using unified pipeline\n",
    "            features = pipeline.compute_features(window_df, lookback=LOOKBACK)\n",
    "            \n",
    "            # Get labels\n",
    "            y_ret = np.array([df.iloc[i][f\"ret_{h}\"] for h in HORIZONS], dtype=np.float32)\n",
    "            y_up = np.array([df.iloc[i][f\"up_{h}\"] for h in HORIZONS], dtype=np.float32)\n",
    "            \n",
    "            if np.any(np.isnan(y_ret)):\n",
    "                continue\n",
    "            \n",
    "            # Store row\n",
    "            rows.append({\n",
    "                \"symbol\": sym,\n",
    "                \"asof\": df.index[i],\n",
    "                \"ohlcv_norm\": features['ohlcv_norm'],  # (120, 5)\n",
    "                \"kronos_emb\": features['kronos_emb'],  # (512,)\n",
    "                \"context\": features['context_vec'],     # (29,)\n",
    "                \"y_ret\": y_ret,  # (3,)\n",
    "                \"y_up\": y_up,    # (3,)\n",
    "            })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {sym}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nTotal samples: {len(rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to parquet\n",
    "df_out = pd.DataFrame(rows)\n",
    "df_out.to_parquet(OUT_PATH, index=False)\n",
    "print(f\"Saved to {OUT_PATH}\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\n=== Dataset Summary ===\")\n",
    "print(f\"Total samples: {len(rows)}\")\n",
    "print(f\"Unique symbols: {df_out['symbol'].nunique()}\")\n",
    "print(f\"Date range: {df_out['asof'].min()} to {df_out['asof'].max()}\")\n",
    "print(f\"OHLCV shape: {rows[0]['ohlcv_norm'].shape if rows else 'N/A'}\")\n",
    "print(f\"Kronos embedding shape: {rows[0]['kronos_emb'].shape if rows else 'N/A'}\")\n",
    "print(f\"Context vector shape: {rows[0]['context'].shape if rows else 'N/A'}\")\n",
    "print(f\"Target horizons: {HORIZONS}\")\n",
    "\n",
    "print(f\"\\n✓ Dataset built using Unified Feature Pipeline\")\n",
    "print(f\"✓ Training and inference now use IDENTICAL features!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}