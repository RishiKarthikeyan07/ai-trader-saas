# AutoPilot Trading Pipelines

This directory contains the core background pipelines for the AutoPilot AI trading system.

## Overview

The AutoPilot system consists of three main pipelines that work together to automate trading:

1. **Daily Brain** - AI-powered stock selection and trade intention generation
2. **Executor** - Automated trade execution and position management
3. **Position Monitor** - Real-time position monitoring and notifications

## Pipelines

### 1. Daily Brain Pipeline (`daily_brain.py`)

**Schedule**: Once daily at 7:00 AM IST (before market opens)

**Purpose**: Generate trade intentions for the day using AI models and screening logic.

**Workflow**:
```
1. Tradability Gate Check
   └─> Fetch all tradable instruments from instruments table

2. PKScreener (Stub)
   └─> Select ~50 candidate stocks
   └─> TODO: Replace with actual PKScreener integration

3. AI Ranking (Stub)
   └─> Assign confidence scores (0.6-0.9)
   └─> TODO: Replace with actual AI model inference

4. Generate Trade Intentions (Top 20)
   └─> Calculate entry zones (current price ± 2%)
   └─> Calculate SL (entry - 3%)
   └─> Calculate TP1 (entry + 5%), TP2 (entry + 8%)
   └─> Assign risk grade (LOW/MEDIUM/HIGH)
   └─> Assign horizon (INTRADAY/SWING/POSITIONAL)

5. Insert into Database
   └─> Upsert into trade_intentions table

6. Update Pipeline Status
   └─> Mark as SUCCESS in pipeline_runs
```

**Output**: 20 trade intentions per day stored in `trade_intentions` table

**Run manually**:
```bash
cd /Users/rishi/Downloads/AI_TRADER
python -m apps.worker.worker.pipelines.daily_brain
```

---

### 2. Executor Pipeline (`executor.py`)

**Schedule**: Every 15 minutes during market hours (9:15 AM - 3:30 PM IST)

**Purpose**: Execute trades based on intentions and manage existing positions.

**Workflow**:
```
1. Market Hours Check
   └─> Skip if outside 9:15 AM - 3:30 PM IST

2. Get Autopilot Users
   └─> Filter: autopilot_enabled=true AND is_active_subscriber=true

3. For Each User:

   a) Risk Limit Checks
      └─> Get user's risk_limits
      └─> Check current exposure vs max_exposure
      └─> Check daily P&L vs max_daily_loss
      └─> Send notification if breach detected

   b) Process Entries (if risk limits OK)
      └─> Load today's trade_intentions (sorted by confidence)
      └─> Check existing positions (avoid duplicates)
      └─> Check max_positions limit
      └─> For each new intention:
          ├─> Calculate position size using risk_per_trade_percent
          ├─> Create BUY order (paper trade)
          ├─> Auto-fill order (stub)
          ├─> Create position record
          └─> Send POSITION_OPENED notification

   c) Process Exits (always run)
      └─> Get all OPEN positions
      └─> For each position:
          ├─> Check current price vs SL/TP1/TP2
          ├─> Check trailing stop conditions
          ├─> Create SELL order if exit triggered
          ├─> Update position to CLOSED
          └─> Send POSITION_CLOSED notification

4. Update Pipeline Status
   └─> Log execution stats in pipeline_runs
```

**Key Features**:
- Position sizing based on risk percentage
- Automatic SL/TP management
- Multi-user support
- Risk limit enforcement
- Paper trading mode (no real broker calls yet)

**Run manually**:
```bash
cd /Users/rishi/Downloads/AI_TRADER
python -m apps.worker.worker.pipelines.executor
```

---

### 3. Position Monitor Pipeline (`position_monitor.py`)

**Schedule**: Every 5 minutes (continuous monitoring)

**Purpose**: Monitor all open positions and send real-time notifications.

**Workflow**:
```
1. Get All Open Positions
   └─> Fetch all positions with status='OPEN'

2. For Each Position:

   a) Update Current Price
      └─> Fetch current market price (stub: simulated)
      └─> TODO: Replace with yfinance or broker API

   b) Calculate Unrealized P&L
      └─> P&L = (current_price - entry_price) × quantity
      └─> P&L% = (current_price - entry_price) / entry_price

   c) Update Database
      └─> Store current_price, unrealized_pnl, unrealized_pnl_percent

   d) Check Trailing Stop
      └─> If enabled, update SL based on highest price
      └─> Trail by 2% from peak

   e) Send Notifications:
      ├─> TP1 hit (price >= tp1)
      ├─> TP2 hit (price >= tp2)
      ├─> SL hit (price <= sl)
      ├─> Large drawdown (P&L% < -10%)
      └─> Large profit (P&L% > +15%)

3. Update Pipeline Status
   └─> Log monitoring stats in pipeline_runs
```

**Notifications Sent**:
- Position milestones (TP1, TP2, SL)
- Risk alerts (large drawdowns)
- Profit opportunities (consider booking)

**Run manually**:
```bash
cd /Users/rishi/Downloads/AI_TRADER
python -m apps.worker.worker.pipelines.position_monitor
```

---

## Database Tables Used

### `trade_intentions`
Generated by daily_brain, consumed by executor.

```sql
- date: Trade date
- canonical_symbol: Stock symbol
- direction: BUY/SELL
- entry_zone_low/high: Entry price range
- sl, tp1, tp2: Stop loss and take profit levels
- confidence: AI model score (0-1)
- risk_grade: LOW/MEDIUM/HIGH
- horizon: INTRADAY/SWING/POSITIONAL
- tags: JSON array of tags
```

### `positions`
Created by executor, monitored by position_monitor.

```sql
- user_id: Owner
- canonical_symbol: Stock symbol
- side: LONG/SHORT
- quantity: Number of shares
- avg_entry_price: Entry price
- current_price: Latest price (updated by monitor)
- unrealized_pnl: Current P&L
- sl, tp1, tp2: Exit levels
- trailing_stop_enabled: Boolean
- trailing_stop_data: JSON with stop data
- status: OPEN/CLOSED/PARTIALLY_CLOSED
- risk_snapshot: JSON with risk params
```

### `orders`
Created by executor for all trades.

```sql
- user_id: Owner
- broker_type: 'paper' for now
- canonical_symbol: Stock symbol
- side: BUY/SELL
- quantity: Number of shares
- order_type: MARKET/LIMIT/SL/SL-M
- price: Order price
- status: PENDING/OPEN/FILLED/CANCELLED/REJECTED
- is_paper_trade: true (no real broker yet)
- trade_intention_id: Link to intention
- position_id: Link to position
```

### `notifications`
Created by executor and position_monitor.

```sql
- user_id: Recipient
- type: ORDER_PLACED/POSITION_OPENED/POSITION_CLOSED/RISK_LIMIT_BREACH
- title: Short title
- message: Detailed message
- severity: INFO/WARNING/ERROR
- read: Boolean flag
```

### `pipeline_runs`
Created by all pipelines for monitoring.

```sql
- type: daily_brain/executor/position_monitor
- status: RUNNING/SUCCESS/FAILED
- started_at, ended_at: Timestamps
- duration_seconds: Execution time
- metadata: JSON with execution stats
- error_message: Error details if failed
```

---

## Configuration

All pipelines use settings from `../config.py`:

```python
# Market Hours (IST)
MARKET_OPEN_HOUR = 9
MARKET_OPEN_MINUTE = 15
MARKET_CLOSE_HOUR = 15
MARKET_CLOSE_MINUTE = 30

# Pipeline Schedules
DAILY_PIPELINE_HOUR = 7  # 7 AM IST
EXECUTOR_INTERVAL_MINUTES = 15
POSITION_MONITOR_INTERVAL_MINUTES = 5

# Supabase Connection
SUPABASE_URL = "..."
SUPABASE_SERVICE_ROLE_KEY = "..."
```

---

## Error Handling

All pipelines include:

1. **Try-Catch Blocks**: Wrap all major operations
2. **Logging**: Comprehensive debug/info/warning/error logs
3. **Pipeline Run Tracking**: Update status in database
4. **Graceful Degradation**: Continue processing other items on single failures
5. **Notifications**: Alert users on critical errors

Example error handling pattern:
```python
try:
    # Create pipeline run
    run_id = await self._create_pipeline_run(started_at)

    # Execute pipeline logic
    result = await self._execute()

    # Update as SUCCESS
    await self._update_pipeline_run(status="SUCCESS", ...)

except Exception as e:
    logger.error(f"Pipeline failed: {e}", exc_info=True)

    # Update as FAILED
    await self._update_pipeline_run(status="FAILED", error_message=str(e))

    raise
```

---

## Current Limitations (Stubs)

The following features are currently stubbed and need replacement:

### Daily Brain
- **PKScreener**: Currently selects random 50 stocks
  - TODO: Integrate actual PKScreener logic
- **AI Ranking**: Assigns random confidence scores
  - TODO: Use trained AI models (Stockformer/TFT/VETO)
- **Price Fetching**: Generates random prices
  - TODO: Fetch actual prices from yfinance/AlphaVantage

### Executor
- **Order Execution**: Auto-fills orders (paper trading)
  - TODO: Integrate BrokerHub for real broker API calls
- **Current Price**: Uses entry price as current price
  - TODO: Fetch real-time prices from market data API

### Position Monitor
- **Price Updates**: Simulates random price movements
  - TODO: Integrate yfinance for real-time data
- **Trailing Stops**: Simple percentage-based logic
  - TODO: Implement PPO-based intelligent trailing

---

## Testing

### Unit Testing
```bash
# Run tests for individual pipelines
pytest apps/worker/tests/test_daily_brain.py
pytest apps/worker/tests/test_executor.py
pytest apps/worker/tests/test_position_monitor.py
```

### Integration Testing
```bash
# Test full pipeline flow
python apps/worker/tests/test_pipeline_integration.py
```

### Manual Testing
```bash
# 1. Run daily brain to generate intentions
python -m apps.worker.worker.pipelines.daily_brain

# 2. Check trade_intentions table
# SELECT * FROM trade_intentions WHERE date = CURRENT_DATE;

# 3. Enable autopilot for a test user
# UPDATE profiles SET autopilot_enabled=true, is_active_subscriber=true WHERE user_id='...';

# 4. Run executor to create positions
python -m apps.worker.worker.pipelines.executor

# 5. Check positions and orders tables
# SELECT * FROM positions WHERE status='OPEN';
# SELECT * FROM orders WHERE is_paper_trade=true;

# 6. Run position monitor
python -m apps.worker.worker.pipelines.position_monitor

# 7. Check notifications
# SELECT * FROM notifications WHERE read=false;
```

---

## Deployment

### Scheduler Setup (Production)

Use a task scheduler like **Celery**, **APScheduler**, or **Temporal** to run pipelines:

```python
# scheduler.py
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apps.worker.worker.pipelines import (
    daily_brain_main,
    executor_main,
    position_monitor_main
)

scheduler = AsyncIOScheduler()

# Daily brain at 7 AM IST
scheduler.add_job(daily_brain_main, 'cron', hour=7, minute=0)

# Executor every 15 min during market hours (9:15 AM - 3:30 PM IST)
scheduler.add_job(executor_main, 'cron', minute='*/15', hour='9-15')

# Position monitor every 5 min
scheduler.add_job(position_monitor_main, 'cron', minute='*/5')

scheduler.start()
```

### Docker Deployment

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY . .

RUN pip install -r requirements.txt

# Run scheduler
CMD ["python", "scheduler.py"]
```

---

## Monitoring & Observability

### Pipeline Run Queries

```sql
-- Check recent pipeline runs
SELECT type, status, started_at, duration_seconds, metadata
FROM pipeline_runs
ORDER BY started_at DESC
LIMIT 20;

-- Failed runs
SELECT *
FROM pipeline_runs
WHERE status = 'FAILED'
ORDER BY started_at DESC;

-- Average execution time
SELECT type, AVG(duration_seconds) as avg_duration
FROM pipeline_runs
WHERE status = 'SUCCESS'
GROUP BY type;
```

### Health Checks

```python
# Check if pipelines are running on schedule
async def check_pipeline_health():
    # Daily brain should have run today
    today_brain = await supabase.table("pipeline_runs")\
        .select("*")\
        .eq("type", "daily_brain")\
        .gte("started_at", date.today().isoformat())\
        .execute()

    if not today_brain.data:
        alert("Daily brain has not run today!")
```

---

## Future Enhancements

1. **Model Integration**
   - Replace AI ranking stub with trained models
   - Add model versioning and A/B testing
   - Implement ensemble predictions

2. **Broker Integration**
   - Connect to real brokers via BrokerHub
   - Handle order rejections and retries
   - Implement order status polling

3. **Advanced Risk Management**
   - Portfolio-level risk limits
   - Correlation-based position sizing
   - Dynamic risk adjustment

4. **Smart Exit Strategies**
   - PPO-based trailing stops
   - Partial profit booking
   - Time-based exits

5. **Performance Analytics**
   - Calculate Sharpe ratio, max drawdown
   - Generate performance reports
   - Benchmark against indices

6. **Alerts & Notifications**
   - SMS/Email notifications
   - Telegram/WhatsApp integration
   - Custom alert rules

---

## Support

For issues or questions, contact the development team or file an issue in the project repository.

**Pipeline Logs**: All pipelines log to stdout. Configure log aggregation in production.

**Database Access**: Pipelines use `SUPABASE_SERVICE_ROLE_KEY` for full database access.
