{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 — Build training dataset + Kronos embeddings (512d)\n",
    "\n",
    "Runs on Colab T4. Pulls yfinance daily bars, builds TF-align/SMC/TA vectors via in-repo preprocessors, encodes Kronos embeddings, and saves `training_data/v1/dataset.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install yfinance pandas numpy pyarrow duckdb torch huggingface_hub tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, pathlib\n",
    "\n",
    "# Locate repo root (works from root or notebooks/)\n",
    "REPO_URL = os.getenv(\"REPO_URL\", \"https://github.com/RishiKarthikeyan07/ai-trader-saas\")\n",
    "REPO_DIR_NAME = os.getenv(\"REPO_DIR\", \"AI_TRADER\")\n",
    "\n",
    "cwd = pathlib.Path().resolve()\n",
    "repo_root = None\n",
    "\n",
    "# Try to find repo root by looking for 'apps' or 'backend' directory\n",
    "for p in [cwd, *cwd.parents]:\n",
    "    if (p / \"apps\").exists() or (p / \"backend\").exists():\n",
    "        repo_root = p\n",
    "        break\n",
    "\n",
    "# If not found, clone the repository\n",
    "if repo_root is None:\n",
    "    target = cwd / REPO_DIR_NAME\n",
    "    if not target.exists():\n",
    "        !git clone $REPO_URL $target.name\n",
    "    repo_root = target\n",
    "    os.chdir(repo_root)\n",
    "else:\n",
    "    os.chdir(repo_root)\n",
    "\n",
    "print(f\"Repo root: {pathlib.Path().resolve()}\")\n",
    "\n",
    "# Add apps/api to Python path so we can import 'app' module\n",
    "api_path = repo_root / \"apps\" / \"api\"\n",
    "if api_path.exists():\n",
    "    sys.path.insert(0, str(api_path))\n",
    "    print(f\"✓ Added to Python path: {api_path}\")\n",
    "else:\n",
    "    # Fallback to backend if apps/api doesn't exist\n",
    "    backend_path = repo_root / \"backend\"\n",
    "    if backend_path.exists():\n",
    "        sys.path.insert(0, str(backend_path))\n",
    "        print(f\"✓ Added to Python path: {backend_path}\")\n",
    "    else:\n",
    "        print(f\"⚠ Warning: Neither {api_path} nor {backend_path} found\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(repo_root / \"training_data/v1\", exist_ok=True)\n",
    "print(f\"✓ Output directory ready: {repo_root / 'training_data/v1'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Python path and import dependencies\n",
    "print(\"Python path (first 3 entries):\")\n",
    "for i, p in enumerate(sys.path[:3]):\n",
    "    print(f\"  [{i}] {p}\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import from local app module\n",
    "from app.ml.preprocess.normalize import (\n",
    "    normalize_ohlcv_120,\n",
    "    build_tf_align_vec,\n",
    "    build_smc_vec,\n",
    "    build_ta_vec,\n",
    ")\n",
    "from app.services.kronos_loader import load_kronos_hf\n",
    "from app.services.feature_engine import compute_ta_features, compute_smc_features\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\n✓ Using device: {device}\")\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "from pathlib import Path\n",
    "\n",
    "LOOKBACK = 120\n",
    "HORIZONS = [3, 5, 10]\n",
    "START = os.getenv(\"DATA_START\", \"2015-01-01\")  # 9y+ history\n",
    "END = os.getenv(\"DATA_END\", None)  # None == today\n",
    "TICKER_FILE = Path(os.getenv(\"TICKER_FILE\", repo_root / \"config/nifty100_yfinance.txt\"))\n",
    "\n",
    "ticker_path = TICKER_FILE if isinstance(TICKER_FILE, Path) else Path(TICKER_FILE)\n",
    "if ticker_path.exists():\n",
    "    with open(ticker_path) as f:\n",
    "        TICKERS = [t.strip() for t in f if t.strip()]\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Ticker file {ticker_path} not found. Provide a yfinance symbol list (one per line, e.g., RELIANCE.NS).\"\n",
    "    )\n",
    "\n",
    "if not TICKERS:\n",
    "    raise ValueError(\"No tickers loaded; check TICKER_FILE\")\n",
    "\n",
    "OUT_PATH = repo_root / \"training_data/v1/dataset.parquet\"\n",
    "print(f\"Using {len(TICKERS)} tickers; saving to {OUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_KEY = os.getenv('ALPHAVANTAGE_API_KEY')\n",
    "\n",
    "def fetch_daily(sym: str) -> pd.DataFrame:\n",
    "    # Prefer AlphaVantage if key present; map NSE tickers like RELIANCE.NS -> NSE:RELIANCE\n",
    "    bases = []\n",
    "    if sym.endswith('.NS') or sym.endswith('.BSE'):\n",
    "        base = sym.split('.')[0]\n",
    "    else:\n",
    "        base = sym\n",
    "    bases = [f'NSE:{base}', f'BSE:{base}', base]\n",
    "\n",
    "    if ALPHA_KEY:\n",
    "        for av_sym in bases:\n",
    "            for attempt in range(4):\n",
    "                try:\n",
    "                    params = {\n",
    "                        'function': 'TIME_SERIES_DAILY_ADJUSTED',\n",
    "                        'symbol': av_sym,\n",
    "                        'outputsize': 'full',\n",
    "                        'apikey': ALPHA_KEY,\n",
    "                    }\n",
    "                    resp = requests.get('https://www.alphavantage.co/query', params=params, timeout=30)\n",
    "                    data = resp.json()\n",
    "                    if 'Note' in data:\n",
    "                        import time\n",
    "                        time.sleep(15)\n",
    "                        continue\n",
    "                    series = data.get('Time Series (Daily)', {})\n",
    "                    if series:\n",
    "                        records = []\n",
    "                        for date, vals in series.items():\n",
    "                            records.append({\n",
    "                                'date': pd.to_datetime(date),\n",
    "                                'open': float(vals['1. open']),\n",
    "                                'high': float(vals['2. high']),\n",
    "                                'low': float(vals['3. low']),\n",
    "                                'close': float(vals['4. close']),\n",
    "                                'volume': float(vals['6. volume']),\n",
    "                            })\n",
    "                        df = pd.DataFrame(records).sort_values('date')\n",
    "                        df.set_index('date', inplace=True)\n",
    "                        if END:\n",
    "                            df = df.loc[(df.index >= pd.to_datetime(START)) & (df.index <= pd.to_datetime(END))]\n",
    "                        else:\n",
    "                            df = df.loc[df.index >= pd.to_datetime(START)]\n",
    "                        if not df.empty:\n",
    "                            return df\n",
    "                except Exception as exc:\n",
    "                    print(f'[warn] AlphaVantage failed for {av_sym} attempt {attempt+1}: {exc}')\n",
    "                import time\n",
    "                time.sleep(15)\n",
    "\n",
    "    # Fallback to Yahoo Finance\n",
    "    last_exc = None\n",
    "    for attempt in range(6):\n",
    "        try:\n",
    "            df = yf.download(\n",
    "                sym,\n",
    "                start=START,\n",
    "                end=END,\n",
    "                interval='1d',\n",
    "                auto_adjust=False,\n",
    "                progress=False,\n",
    "                threads=False,\n",
    "            )\n",
    "            if not df.empty:\n",
    "                break\n",
    "        except Exception as exc:\n",
    "            last_exc = exc\n",
    "        import time\n",
    "        time.sleep(2.0 * (attempt + 1))\n",
    "    else:\n",
    "        if last_exc:\n",
    "            print(f'[warn] {sym} failed after retries: {last_exc}')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = df.rename(columns=str.lower)[['open', 'high', 'low', 'close', 'volume']].dropna()\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index': 'date', 'Date': 'date'}, inplace=True)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for h in HORIZONS:\n",
    "        out[f'ret_{h}'] = (out['close'].shift(-h) / out['close']) - 1.0\n",
    "        out[f'up_{h}'] = (out[f'ret_{h}'] > 0).astype(np.int32)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction helpers aligned with backend\n",
    "def _prep_window(window: pd.DataFrame) -> pd.DataFrame | None:\n",
    "    if window.empty:\n",
    "        return None\n",
    "    df = window.copy()\n",
    "    df.columns = [str(c).lower() for c in df.columns]\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "    df = df.sort_index()\n",
    "    required = ['open', 'high', 'low', 'close', 'volume']\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_alignment(window: pd.DataFrame) -> dict:\n",
    "    base = _prep_window(window)\n",
    "    if base is None:\n",
    "        return {'monthly_bias': 0.0, 'weekly_bias': 0.0, 'daily_bias': 0.0, 'h4_align': 0.0, 'h1_align': 0.0}\n",
    "    core = base[['open', 'high', 'low', 'close', 'volume']]\n",
    "    wk = core.resample('W-FRI').agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last', 'volume': 'sum'}).dropna()\n",
    "    mo = core.resample('ME').agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last', 'volume': 'sum'}).dropna()\n",
    "    h4 = core.resample('4h').agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last', 'volume': 'sum'}).dropna()\n",
    "    h1 = core.copy()  # already 1H if provided; with daily data it's sparse but harmless\n",
    "\n",
    "    def bias(df: pd.DataFrame) -> float:\n",
    "        enriched = compute_ta_features(df)\n",
    "        if enriched.empty:\n",
    "            return 0.0\n",
    "        latest = enriched.iloc[-1]\n",
    "        return 1.0 if latest.get('ema_fast', 0) > latest.get('ema_slow', 0) else -1.0\n",
    "\n",
    "    return {\n",
    "        'monthly_bias': bias(mo),\n",
    "        'weekly_bias': bias(wk),\n",
    "        'daily_bias': bias(core),\n",
    "        'h4_align': bias(h4),\n",
    "        'h1_align': bias(h1),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_feature_dict(window: pd.DataFrame) -> dict:\n",
    "    base = _prep_window(window)\n",
    "    if base is None:\n",
    "        return {}\n",
    "    enriched = compute_ta_features(base)\n",
    "    enriched = compute_smc_features(enriched)\n",
    "    if enriched.empty:\n",
    "        return {}\n",
    "    latest = enriched.iloc[-1].to_dict()\n",
    "    latest.update({\n",
    "        'open': float(base.iloc[-1]['open']),\n",
    "        'high': float(base.iloc[-1]['high']),\n",
    "        'low': float(base.iloc[-1]['low']),\n",
    "        'close': float(base.iloc[-1]['close']),\n",
    "        'volume': float(base.iloc[-1]['volume']),\n",
    "    })\n",
    "    return latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kronos 512d encoder\n",
    "kronos = load_kronos_hf(device=device, max_context=512)\n",
    "\n",
    "def kronos_embed(batch_norm: np.ndarray) -> np.ndarray:\n",
    "    # batch_norm: (B,120,5)\n",
    "    x = torch.tensor(batch_norm, dtype=torch.float32, device=device)\n",
    "    if x.shape[-1] == 5:  # pad amount channel if tokenizer expects 6\n",
    "        amt = torch.zeros(x.shape[0], x.shape[1], 1, device=device)\n",
    "        x = torch.cat([x, amt], dim=-1)\n",
    "    z = kronos.tokenizer.embed(x)\n",
    "    if isinstance(z, tuple):\n",
    "        z = z[0]\n",
    "    emb = z.mean(dim=1).detach().cpu().numpy().astype(np.float32)\n",
    "    if emb.shape[1] < 512:\n",
    "        pad = np.zeros((emb.shape[0], 512 - emb.shape[1]), dtype=np.float32)\n",
    "        emb = np.concatenate([emb, pad], axis=1)\n",
    "    elif emb.shape[1] > 512:\n",
    "        emb = emb[:, :512]\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "rows = []\n",
    "\n",
    "for sym in tqdm(TICKERS):\n",
    "    # Throttle API calls for Yahoo Finance\n",
    "    time.sleep(2.0)\n",
    "\n",
    "    df = fetch_daily(sym)\n",
    "    if df.empty or len(df) < LOOKBACK + max(HORIZONS) + 10:\n",
    "        continue\n",
    "    df = add_labels(df)\n",
    "\n",
    "    batch_ohlcv = []\n",
    "    batch_meta = []\n",
    "\n",
    "    for i in range(LOOKBACK - 1, len(df) - max(HORIZONS)):\n",
    "        window = df.iloc[i - LOOKBACK + 1 : i + 1]\n",
    "        ohlcv = window[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].values.astype(np.float32)\n",
    "        if ohlcv.shape[0] != LOOKBACK:\n",
    "            continue\n",
    "        norm = normalize_ohlcv_120(ohlcv)\n",
    "        alignment = build_tf_align_vec(compute_alignment(window))\n",
    "        feat_dict = compute_feature_dict(window)\n",
    "        smc_vec = build_smc_vec(feat_dict)\n",
    "        ta_vec = build_ta_vec(feat_dict)\n",
    "        context = np.concatenate([alignment, smc_vec, ta_vec]).astype(np.float32)\n",
    "        y_ret = np.array([df.iloc[i][f\"ret_{h}\"] for h in HORIZONS], dtype=np.float32)\n",
    "        y_up = np.array([df.iloc[i][f\"up_{h}\"] for h in HORIZONS], dtype=np.float32)\n",
    "        if np.any(np.isnan(y_ret)):\n",
    "            continue\n",
    "        batch_ohlcv.append(norm)\n",
    "        batch_meta.append((sym, df.index[i], context, y_ret, y_up))\n",
    "\n",
    "        # Process in batches of 64 for GPU efficiency\n",
    "        if len(batch_ohlcv) >= 64:\n",
    "            kron = kronos_embed(np.stack(batch_ohlcv, axis=0))\n",
    "            for (m_sym, m_date, m_ctx, m_ret, m_up), m_emb, m_ohlcv in zip(batch_meta, kron, batch_ohlcv):\n",
    "                rows.append(\n",
    "                    {\"symbol\": m_sym, \"asof\": m_date, \"ohlcv_norm\": m_ohlcv, \"kronos_emb\": m_emb, \"context\": m_ctx, \"y_ret\": m_ret, \"y_up\": m_up}\n",
    "                )\n",
    "            batch_ohlcv, batch_meta = [], []\n",
    "\n",
    "    # Process remaining samples\n",
    "    if batch_ohlcv:\n",
    "        kron = kronos_embed(np.stack(batch_ohlcv, axis=0))\n",
    "        for (m_sym, m_date, m_ctx, m_ret, m_up), m_emb, m_ohlcv in zip(batch_meta, kron, batch_ohlcv):\n",
    "            rows.append(\n",
    "                {\"symbol\": m_sym, \"asof\": m_date, \"ohlcv_norm\": m_ohlcv, \"kronos_emb\": m_emb, \"context\": m_ctx, \"y_ret\": m_ret, \"y_up\": m_up}\n",
    "            )\n",
    "\n",
    "print(f\"\\nTotal samples: {len(rows)}\")\n",
    "df_out = pd.DataFrame(rows)\n",
    "df_out.to_parquet(OUT_PATH, index=False)\n",
    "print(f\"Saved to {OUT_PATH}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\n=== Dataset Summary ===\")\n",
    "print(f\"Total samples: {len(rows)}\")\n",
    "print(f\"Unique symbols: {df_out['symbol'].nunique()}\")\n",
    "print(f\"Date range: {df_out['asof'].min()} to {df_out['asof'].max()}\")\n",
    "print(f\"OHLCV shape: {rows[0]['ohlcv_norm'].shape if rows else 'N/A'}\")\n",
    "print(f\"Kronos embedding shape: {rows[0]['kronos_emb'].shape if rows else 'N/A'}\")\n",
    "print(f\"Context vector shape: {rows[0]['context'].shape if rows else 'N/A'}\")\n",
    "print(f\"Target horizons: {HORIZONS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
